{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e496fc2-2c8b-4bca-bef9-4350c0ebd4c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424d5791-a4e9-4423-8d3a-e890d39efe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Gagana Methmal\n",
      "[nltk_data]     (GM)\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gagana Methmal\n",
      "[nltk_data]     (GM)\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Downloading the NLTK stuff.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8a4fa-bc2e-4746-8b17-406dc46d0f0a",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25caddbc-09f0-4dd5-bd00-f19f9846a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess.\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79d3ab-0e31-46c6-807c-7d2da566e470",
   "metadata": {},
   "source": [
    "## Loading Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562560c5-2af5-41ac-b70f-3af91e52e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['ID', 'Statement', 'Status'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset.\n",
    "df = pd.read_csv('Mental Health Sentiments.csv')\n",
    "print(\"Columns in the dataset:\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a652cc-6e5a-4099-8bff-e65a46099b2a",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fac42b2-bedd-4229-924e-f124276f8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing.\n",
    "df['Statement'] = df['Statement'].fillna('Missing Statement').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7561f-db43-4236-8ae5-0d55ec8f5b5d",
   "metadata": {},
   "source": [
    "## Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a3d07d-276b-4655-9cd4-996d813f39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Labels.\n",
    "label_encoder = LabelEncoder()\n",
    "df['Status'] = label_encoder.fit_transform(df['Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14591610-39f6-4882-a3b1-ea2cbe2e8183",
   "metadata": {},
   "source": [
    "## Checking class balance before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e92659-9601-4522-9661-6f46967fa553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution Before Balancing:\n",
      "Status\n",
      "3    16351\n",
      "2    15404\n",
      "6    10652\n",
      "0     3888\n",
      "1     2877\n",
      "5     2669\n",
      "4     1201\n",
      "7        3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking Class Balance.\n",
    "print(\"\\nClass Distribution Before Balancing:\")\n",
    "print(df['Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf850b1b-aa5c-4701-bda0-499652bb03c9",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Datset with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7dce3bf-6de1-489f-8a8f-ae1d4a54a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Balancing:\n",
      "Status\n",
      "0    16351\n",
      "3    16351\n",
      "2    16351\n",
      "6    16351\n",
      "7    16351\n",
      "5    16351\n",
      "1    16351\n",
      "4    16351\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle Imbalanced Dataset with SMOTE.\n",
    "X = df['Statement']\n",
    "y = df['Status']\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Handle rare minority classes using an adjusted SMOTE.\n",
    "smote = SMOTE(random_state=42, k_neighbors=2)  \n",
    "try:\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "except ValueError as e:\n",
    "    print(\"SMOTE Error:\", e)\n",
    "    print(\"Adjusting the dataset or resampling parameters may be required.\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nClass Distribution After Balancing:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199ef3b-144c-44b0-a99c-b200eef347f0",
   "metadata": {},
   "source": [
    "## Splitting for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa71069-aba1-4a33-b942-1b8093a293c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb1bbe-8fd7-4825-8f43-33e8c13f92e4",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17513d2b-cacb-413a-889b-c85d3635d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning for Logistic Regression.\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2'],\n",
    "}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cbc82-b939-4514-b6a5-0f05ca19637b",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54dc1e79-4155-4944-b3c4-a9d5b18d1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the model: 89.78671355400964 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      3250\n",
      "           1       0.99      0.95      0.97      3338\n",
      "           2       0.83      0.68      0.75      3256\n",
      "           3       0.87      0.90      0.88      3302\n",
      "           4       1.00      0.90      0.95      3239\n",
      "           5       0.97      0.97      0.97      3204\n",
      "           6       0.74      0.81      0.77      3313\n",
      "           7       0.85      1.00      0.92      3260\n",
      "\n",
      "    accuracy                           0.90     26162\n",
      "   macro avg       0.90      0.90      0.90     26162\n",
      "weighted avg       0.90      0.90      0.90     26162\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3169    1   11   17    1    5    5   41]\n",
      " [   8 3175   12   10    0    2    5  126]\n",
      " [  80   30 2223  150    8   44  721    0]\n",
      " [  22    3   70 2971    0   30  205    1]\n",
      " [   0    0    0    0 2928    0    0  311]\n",
      " [   3    2    6    7    1 3094    4   87]\n",
      " [   5   10  350  265    1   11 2670    1]\n",
      " [   0    0    0    0    0    0    0 3260]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model.\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy of the model:\", accuracy * 100, \"%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887c120-c2a5-49ab-8540-9e67682bb538",
   "metadata": {},
   "source": [
    "## Predict for real-time use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3162642b-c260-44ac-85c4-557131a46f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Class for the User Input: ['Anxiety']\n",
      "\n",
      "Confidence Scores for Each Class: [[3.64426242e-01 4.88491776e-03 4.81330080e-02 9.73443731e-02\n",
      "  4.49041501e-03 1.90853826e-01 2.89820173e-01 4.70453513e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Predict with Probabilities for Real-Time Use.\n",
    "custom_input = [\"I'm feeling overwhelmed.\"]\n",
    "custom_input_preprocessed = [preprocess_text(sentence) for sentence in custom_input]\n",
    "custom_input_transformed = vectorizer.transform(custom_input_preprocessed)\n",
    "custom_prediction = best_model.predict(custom_input_transformed)\n",
    "custom_probabilities = best_model.predict_proba(custom_input_transformed)\n",
    "\n",
    "predicted_label = label_encoder.inverse_transform(custom_prediction)\n",
    "print(\"\\nPredicted Class for the User Input:\", predicted_label)\n",
    "print(\"\\nConfidence Scores for Each Class:\", custom_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc304e-9b6f-4d2a-8bcf-02e3c9f132f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
